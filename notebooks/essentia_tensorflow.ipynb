{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e546fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import essentia\n",
    "# there are two operating modes in essentia which (mostly) have the same algorithms\n",
    "# they are accessible via two submodules:\n",
    "# import essentia.standard\n",
    "# import essentia.streaming\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path(\"../src/technotaggr/\")\n",
    "sys.path.append(str(module_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4669aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../song_data/music-library-analysis-dataset/1-01 Funken.aiff')]\n",
      "..\n",
      "../song_data/music-library-analysis-dataset\n",
      "../src/technotaggr/models\n"
     ]
    }
   ],
   "source": [
    "root = Path('..')\n",
    "print(list(Path(\"..\").rglob(\"*Funken.aiff\")))\n",
    "data = root / \"song_data\" / \"music-library-analysis-dataset\"\n",
    "models = root /\"src\" / \"technotaggr\" / \"models\"\n",
    "print(root)\n",
    "print(data)\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2145e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mishkin/Desktop/gitclones/technotaggr/song_data/music-library-analysis-dataset/1-01 Funken.aiff\n"
     ]
    }
   ],
   "source": [
    "print(Path(data / \"1-01 Funken.aiff\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e1ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name_0': '37 - Neverland (Aparde Remix).mp3',\n",
       " 'name_1': 'Intensity_Fluctuations_-_Ø_[Phase]_(Setaoc_Mass_remix).mp3',\n",
       " 'name_2': 'Artaphine - Toru Ikemoto - _080 [Artaphine Premiere].mp3',\n",
       " 'name_4': '01 Testify.flac',\n",
       " 'name_5': '01 Heal My Soul.aiff',\n",
       " 'name_6': '01 Power to the Soul.aiff',\n",
       " 'name_7': 'Aus_der_Tiefe_der_Zeit_-_Obscure_Shape,_SHDW_(Original_Mix).mp3',\n",
       " 'name_8': '01. Eh Wut.flac',\n",
       " 'name_9': '1-02 Tide.aiff',\n",
       " 'name_10': 'Last_Charms_-_Hyden_(Alarico_Remix)_(SYEP009).mp3',\n",
       " 'name_11': '03 For Marco.aiff',\n",
       " 'name_12': '10 - DJ HEARTSTRING - VISION OF ECSTASY.mp3',\n",
       " 'name_13': 'Kolter - Bob Marley - Could you be loved (Kolter Edit).aiff',\n",
       " 'name_14': 'Cyclo_-_Traumer_(Original_Mix).mp3',\n",
       " 'name_15': 'Uncertain, Alarico - Absence (Original Mix).aiff',\n",
       " 'name_16': '1-01 Funken.aiff',\n",
       " 'name_17': '1-02 Bashment Boogie.aiff',\n",
       " 'name_18': 'in_aeternam.aiff',\n",
       " 'name_19': 'What To Do (&ME Remix) - Guy Gerber .aiff',\n",
       " 'name_20': '1-02 We Will Bring It Back.aiff',\n",
       " 'name_21': '1-01 Neck Carver.aiff',\n",
       " 'name_22': 'Shook_Part_3_-_Nick_Morgan_(Original_Mix).mp3',\n",
       " 'name_23': '01 Early Morning Acceleration.aiff',\n",
       " 'name_24': '01 Presence.aiff',\n",
       " 'name_25': 'CAPP STREET PROJECT - VOL. 4 -FREE DL- - 02 Brick, Skiis - BADBOY.aiff',\n",
       " 'name_26': 'HYPNOSIS (Extended Mix) - A-BO, Ero808 .aiff',\n",
       " 'name_27': '29 - BT Premiere： CSP - Funk [ASWVA001].mp3',\n",
       " 'name_28': '01. Only The Gods (feat. Anabel Englund) (Original Mix).wav',\n",
       " 'name_29': '33 - GTG Premiere ｜ OFF ⧸ GRID - Distant Thoughts [MR020].mp3',\n",
       " 'name_30': 'Thinking_Of_You_feat._Camille_Safiya_-_Serge_Devant,_Camille_Safiya,_Damiano_C_(Serge_Devant_s_Floor_Cut).aiff'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = {}\n",
    "for i, f in enumerate(data.iterdir()):\n",
    "    if f.name != '.DS_Store':\n",
    "        names[f\"name_{i}\"] = f.name\n",
    "display(names)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c06ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `/Users/mishkin/Desktop/gitclones/technotaggr/src/technotaggr/models/feature-extractors/musicnn/msd-musicnn-1/msd-musicnn-1.pb`\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764563767.179129 23145599 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "[   INFO   ] TensorflowPredict: Successfully loaded graph file: `/Users/mishkin/Desktop/gitclones/technotaggr/src/technotaggr/models/classification-heads/mood_happy/mood_happy-msd-musicnn-1.pb`\n"
     ]
    }
   ],
   "source": [
    "from essentia.standard import MonoLoader, TensorflowPredictMusiCNN, TensorflowPredict2D\n",
    "# name = \"1-01 Funken.aiff\"\n",
    "audio_path = Path(data / names[\"name_0\"]).as_posix()\n",
    "embedded_model_path = models / \"feature-extractors\" / \"musicnn\" / \"msd-musicnn-1\" / \"msd-musicnn-1.pb\"\n",
    "classifier_model_path = models / \"classification-heads\" / \"mood_happy\" / \"mood_happy-msd-musicnn-1.pb\"\n",
    "audio = MonoLoader(filename= audio_path, sampleRate=16000, resampleQuality=4)()\n",
    "embedding_model = TensorflowPredictMusiCNN(graphFilename=embedded_model_path, output=\"model/dense/BiasAdd\")\n",
    "embeddings = embedding_model(audio)\n",
    "\n",
    "model = TensorflowPredict2D(graphFilename=classifier_model_path, output=\"model/Softmax\")\n",
    "predictions = model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f780ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d510e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio.shape) # number of samples\n",
    "print(f\"duration is {int(audio.shape[0]) / 16000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(models_json /\"msd-musicnn-1.json\").as_posix(), 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "print(metadata['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "activations = TensorflowPredictMusiCNN(graphFilename=embedded_model_path)(audio)\n",
    "ig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.matshow(activations.T, aspect='auto')\n",
    "ax.set_yticks(range(len(metadata['classes'])))\n",
    "ax.set_yticklabels(metadata['classes'])\n",
    "ax.set_xlabel('patch number')\n",
    "ax.xaxis.set_ticks_position('bottom')   \n",
    "plt.title(f'Tag activations for {names[\"name_0\"]}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ce031",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = TensorflowPredictMusiCNN(graphFilename=embedded_model_path, output='model/dense_1/BiasAdd')(audio)\n",
    "ig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.matshow(embeddings.T, aspect='auto')\n",
    "ax.xaxis.set_ticks_position('bottom')   \n",
    "ax.set_xlabel('patch number')\n",
    "plt.title('Embeddings')\n",
    "plt.show()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f0ac8",
   "metadata": {},
   "source": [
    "### Getting Sample Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rates = {}\n",
    "for i, f in enumerate(data.iterdir()):\n",
    "    if f.suffix in ['.wav', '.aiff', '.aif', '.aifc', '.flac', '.mp3']:\n",
    "        sample_rates[f\"{f.name}\"] = librosa.get_samplerate(f.as_posix())\n",
    "        \n",
    "display(sample_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Read essentia paper on muscinn -> get general idea of architecture, intermediate features, and output\n",
    "# 2 get the standalone musicnn repo -> go through the musicnn notebook tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ae3dd",
   "metadata": {},
   "source": [
    "# Testing Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4751442",
   "metadata": {},
   "source": [
    "### DEAM Arousal/Valence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = Path(data / names[\"name_5\"]).as_posix()\n",
    "embedded_model_path = Path(models / \"msd-musicnn-1.pb\").as_posix()\n",
    "classifier_model_path = Path(models / \"deam-msd-musicnn-2.pb\").as_posix()\n",
    "audio = MonoLoader(filename= audio_path, sampleRate=16000, resampleQuality=4)()\n",
    "embedding_model = TensorflowPredictMusiCNN(graphFilename= embedded_model_path, output=\"model/dense/BiasAdd\")\n",
    "embeddings = embedding_model(audio)\n",
    "model = TensorflowPredict2D(graphFilename=classifier_model_path, output=\"model/Identity\")\n",
    "predictions = model(embeddings)\n",
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0bf7fe",
   "metadata": {},
   "source": [
    "- Get BPM of the song\n",
    "- calculate time for 4 beats\n",
    "- get 1 bar -> 4x 4 time\n",
    "-> use this as input to Essentia Model for Patch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatchSizeforBar(audio_file):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571db994",
   "metadata": {},
   "source": [
    "### View Patch size for bar\n",
    "- display the beat locations from global BMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in audio file at desred sample rate\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fad585",
   "metadata": {},
   "source": [
    "### Assess BPM of song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70accf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from essentia.standard import MonoLoader, TempoCNN, RhythmExtractor2013\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3, figsize=(10, 8))\n",
    "fig.text(0.5, 0.04, \"Time(samples)\", ha= 'center')\n",
    "axs[0].set_title(\"Audio waveform and the estimated beat positions\")\n",
    "sr = 11025\n",
    "audio_path = Path(data / names[\"name_1\"]).as_posix()\n",
    "duration = 5\n",
    "audio_slice = audio_11khz[:sr*duration]\n",
    "print(audio_path)\n",
    "print(audio_11khz)\n",
    "\n",
    "\n",
    "classifier_model_path_1 = Path(models /'deepsquare-k16-3.pb').as_posix()\n",
    "audio_11khz = MonoLoader(filename= audio_path, sampleRate=sr, resampleQuality= 4)()\n",
    "global_bpm_1, local_bpm, local_probs = TempoCNN(graphFilename= classifier_model_path_1)(audio_11khz)\n",
    "print('song BPM from deepsquare: {}'.format(global_bpm_1))\n",
    "print(local_bpm)\n",
    "bps_1 = global_bpm_1/60\n",
    "spb_1 = sr/bps_1\n",
    "axs[0].plot(audio_slice)\n",
    "markers = np.arange(0,len(audio_slice), step=spb_1)\n",
    "for marker in markers:\n",
    "    axs[0].axvline(x=marker, color='red', alpha =.5)\n",
    "\n",
    "\n",
    "classifier_model_path_2 = Path(models /'deeptemp-k16-3.pb').as_posix()\n",
    "global_bpm_2, local_bpm, local_probs = TempoCNN(graphFilename= classifier_model_path_2)(audio_11khz)\n",
    "print('song BPM from deeptemp: {}'.format(global_bpm_2))\n",
    "print(local_bpm)\n",
    "axs[1].plot(audio_slice)\n",
    "bps_1 = global_bpm_2/60\n",
    "spb_2 = sr/bps_1\n",
    "markers = np.arange(0,len(audio_slice), step=spb_2)\n",
    "for marker in markers:\n",
    "    axs[1].axvline(x=marker, color='red', alpha =.5)\n",
    "\n",
    "bpm, beats, beats_confidence, _, beats_intervals = RhythmExtractor2013(method=\"multifeature\")(audio_11khz)\n",
    "print(\"BPM:\", bpm)\n",
    "# print(\"Beat positions (sec.):\", beats)\n",
    "print(\"Beat estimation confidence:\", beats_confidence)\n",
    "my_iter = iter(beats)\n",
    "beat = 0\n",
    "axs[2].plot(audio_slice)\n",
    "while beat <= 5:\n",
    "    axs[2].axvline(x=beat*sr, color='red',alpha =.5)\n",
    "    beat = next(my_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49600df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "import mutagen\n",
    "from mutagen import File\n",
    "from mutagen.id3 import ID3\n",
    "from mutagen.mp3 import MP3\n",
    "from mutagen.aiff import AIFF\n",
    "from mutagen.flac import FLAC\n",
    "from mutagen.wave import WAVE\n",
    "from mutagen.mp4 import MP4\n",
    "\n",
    "def _parse_number(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    low = s.lower()\n",
    "    for sep in [\"bpm=\", \"=\", \":\", \";\", \",\"]:\n",
    "        if sep in low:\n",
    "            s = low.split(sep)[-1].strip()\n",
    "            break\n",
    "    for token in [\" bpm\", \" beats/min\", \" beats per minute\"]:\n",
    "        if s.lower().endswith(token):\n",
    "            s = s[:-len(token)].strip()\n",
    "    for split_on in [\"/\", \" \"]:\n",
    "        if split_on in s:\n",
    "            s = s.split(split_on)[0].strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _get_bpm_from_id3(id3: ID3):\n",
    "    if not id3:\n",
    "        return None\n",
    "    f = id3.get('TBPM')\n",
    "    if f and getattr(f, 'text', None):\n",
    "        n = _parse_number(f.text[0])\n",
    "        if n is not None:\n",
    "            return n\n",
    "    for t in id3.getall('TXXX') or []:\n",
    "        desc = getattr(t, 'desc', '') or ''\n",
    "        if desc.lower() in ('bpm', 'tempo', 'beats_per_minute', 'beats per minute'):\n",
    "            vals = getattr(t, 'text', [])\n",
    "            if vals:\n",
    "                n = _parse_number(vals[0])\n",
    "                if n is not None:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def _get_bpm_from_vorbis(tags):\n",
    "    if not tags:\n",
    "        return None\n",
    "    preferred_keys = [\n",
    "        \"bpm\", \"tempo\", \"beats_per_minute\", \"beats per minute\",\n",
    "        \"initial bpm\", \"bpm (beats per minute)\"\n",
    "    ]\n",
    "    for want in preferred_keys:\n",
    "        for k in tags.keys():\n",
    "            if k.lower() == want:\n",
    "                vals = tags.get(k) or []\n",
    "                if not isinstance(vals, list):\n",
    "                    vals = [vals]\n",
    "                for v in vals:\n",
    "                    n = _parse_number(v)\n",
    "                    if n is not None:\n",
    "                        return n\n",
    "    for k in tags.keys():\n",
    "        kl = k.lower()\n",
    "        if \"bpm\" in kl or kl == \"tempo\":\n",
    "            vals = tags.get(k) or []\n",
    "            if not isinstance(vals, list):\n",
    "                vals = [vals]\n",
    "            for v in vals:\n",
    "                n = _parse_number(v)\n",
    "                if n is not None:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def _as_mapping(obj):\n",
    "    \"\"\"Return obj if it's mapping-like (dict-ish), else None.\"\"\"\n",
    "    if isinstance(obj, Mapping):\n",
    "        return obj\n",
    "    if hasattr(obj, \"keys\") and hasattr(obj, \"get\") and hasattr(obj, \"__contains__\"):\n",
    "        return obj\n",
    "    return None\n",
    "\n",
    "def _get_bpm_from_riff_info(tags):\n",
    "    \"\"\"Handle RIFF INFO tags (e.g., IBPM). Accepts mapping-like only.\"\"\"\n",
    "    tagmap = _as_mapping(tags)\n",
    "    if not tagmap:\n",
    "        return None\n",
    "    for key in ('IBPM', 'BPM', 'BPM '):\n",
    "        if key in tagmap:\n",
    "            v = tagmap.get(key)\n",
    "            if isinstance(v, list) and v:\n",
    "                v = v[0]\n",
    "            n = _parse_number(v)\n",
    "            if n is not None:\n",
    "                return n\n",
    "    return None\n",
    "\n",
    "def getBPM(audio_file):\n",
    "    audio = File(audio_file)\n",
    "    if audio is None:\n",
    "        return None\n",
    "\n",
    "    # MP4 (M4A): 'tmpo' atom (list of ints)\n",
    "    if isinstance(audio, MP4):\n",
    "        vals = audio.tags.get('tmpo') if audio.tags else None\n",
    "        if vals:\n",
    "            return float(vals[0])\n",
    "\n",
    "    # MP3 and AIFF (often ID3)\n",
    "    if isinstance(audio, (MP3, AIFF)):\n",
    "        return _get_bpm_from_id3(audio.tags if hasattr(audio, 'tags') else None)\n",
    "\n",
    "    # WAV: could have ID3 or RIFF INFO\n",
    "    if isinstance(audio, WAVE):\n",
    "        # 1) ID3-in-WAV\n",
    "        if isinstance(audio.tags, ID3):\n",
    "            n = _get_bpm_from_id3(audio.tags)\n",
    "            if n is not None:\n",
    "                return n\n",
    "        # 2) RIFF INFO (mapping-like only). Do NOT pass audio.info (WaveStreamInfo)\n",
    "        n = _get_bpm_from_riff_info(getattr(audio, 'tags', None))\n",
    "        if n is not None:\n",
    "            return n\n",
    "        return None\n",
    "\n",
    "    # FLAC: Vorbis comments\n",
    "    if isinstance(audio, FLAC):\n",
    "        return _get_bpm_from_vorbis(audio.tags)\n",
    "\n",
    "    # Other containers (e.g., OGG/Opus) – try Vorbis-like handling\n",
    "    tags = getattr(audio, 'tags', None)\n",
    "    n = _get_bpm_from_vorbis(_as_mapping(tags) or tags)\n",
    "    if n is not None:\n",
    "        return n\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe of all predictions, and save to resultsd\n",
    "import mutagen\n",
    "import math\n",
    "results = {}\n",
    "sr =11025\n",
    "results['TempoCNN-deepsquare'] = []\n",
    "results['Tempo-CNN-deeptempt'] = []\n",
    "results['RhythmExtractor2013'] = []\n",
    "results['Original'] = []\n",
    "classifier_model_path_1 = Path(models /'deepsquare-k16-3.pb').as_posix()\n",
    "classifier_model_path_2 = Path(models /'deeptemp-k16-3.pb').as_posix()\n",
    "\n",
    "                        \n",
    "                 \n",
    "for pos in range(len(names)):\n",
    "    if pos ==3:\n",
    "        continue\n",
    "    audio_path = Path(data / names[f\"name_{pos}\"]).as_posix()\n",
    "    audio_11khz = MonoLoader(filename= audio_path, sampleRate=sr, resampleQuality= 4)()\n",
    "    global_bpm_1, _,_ = TempoCNN(graphFilename= classifier_model_path_1)(audio_11khz)\n",
    "    global_bpm_2, _ ,_ = TempoCNN(graphFilename= classifier_model_path_2)(audio_11khz) \n",
    "    bpm,_,_,_,_ = RhythmExtractor2013(method=\"multifeature\")(audio_11khz)\n",
    "    results['TempoCNN-deepsquare'].append(global_bpm_1)\n",
    "    results['Tempo-CNN-deeptempt'].append(global_bpm_2)\n",
    "    results['RhythmExtractor2013'].append(math.ceil(bpm))\n",
    "    results['Original'].append(getBPM(audio_path))\n",
    "\n",
    "    \n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(embedding_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
